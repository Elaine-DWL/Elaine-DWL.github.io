<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深度学习之CNN基础（1）]]></title>
    <url>%2F2018%2F07%2F05%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BCNN%E5%9F%BA%E7%A1%80%20(1)%2F</url>
    <content type="text"><![CDATA[深度学习之CNN基础本文主要是对CS231N课程中有关CNN的各种层进行一个总结，截图来自课程或CS231N网易云课堂的视频。 卷积层卷积层是CNN中最基础的层，在DNN中，我们是考虑对数据的线性变换，而这里，CNN则考虑的是对图像（张量数据）的线性变换，DNN中并没有考虑空间信息，而CNN中的卷积操作，更好地来处理图像的这种空间性。 什么是卷积核？卷积层涉及到的主要是用卷积核对图像进行卷积处理，卷积核说白了，就是三维的矩阵，用这个三维矩阵在feature map 上进行滑窗处理。 注意：用卷积核对图像（feature map）进行处理，输出的feature map的维度 和 卷积核的个数相等。比如说，用5*5*3（这里3是卷积核的数量）的卷积核对图像进行操作，输出的feature map 的维度一定是3. 卷积运算 如上图所示，图像大小是（32*32*3），用大小为（5*5*5）的卷积核处理，得到（28*28*5）的输出。 由上图可以知道，卷积输出的深度(激活映射的层数)和卷积核的个数相等。每个卷积核都要在输入的整个深度上做卷积。 用一个有5*5*3个参数值的滤波器，然后以相同的方式滑动(卷积核)，划过整个的输入部分，从而得到一张激活映射(activation map). 这里，b是bias，这里的w是指将卷积核展开得到的一个一维向量。所以说，卷积操作本质还是一种线性变换。每一个卷积核会产生一个activation map，就是说生成的激活映射的层数和卷积核的个数 一样。 那么，我们知道，卷积操作的输出维度和卷积核的个数相等，但是，如何来确定生成的feature map的长宽呢？ 在无边缘填充的情况下，对一张输入大小为N*N的图像，使用大小为F*F的卷积核以步长为stride对它做卷积，输出图像的大小为 (N - F) / stride + 1(结果必须是整数，所以步长的选择必须适当) 然而，在实际的操作中，一般会对待进行卷积操作的图像进行边缘填充（填充方法可以是常用的零填充或者是其它），所以此时在计算输出图像的大小尺寸时，仍然时使用的之前的公式：(N - F) / stride + 1，但是N要进行更新，应该是填充后的大小。如下图所示： 思考 为什么要做零填充？ 因为我们想尽可能地保持全尺寸输出。如果不做零填充或者其它任何形式的填充，那么输出图像的尺寸会迅速减小，这样是不好的，可能会损失很多的信息，最后只能用很少的值来表示原始图像。关于图像的边角信息也会损失的更多。 池化层 池化层的作用：下采样。 池化层的参数：没有需要学习的参数。需要设置池化的步长。 池化层的类型：最大池化、平均池化等。 BN层BN层的全称是Batch Normalization(批归一化层)，这里的batch，顾名思义，指的是我们mini-batch SGD中的 batch。 BN层操作首先是计算出mini-batch中各个特征的均值和方差，然后对mini-batch中的每个特征x_i 进行标准化处理，最后利用缩放参数$\gamma$和偏移参数$\ \beta$对特征$x^i$进一步处理后，来代替最初的输入特征值。也就是说对于每个神经元都会有一对参数值(（γ和β）通过训练来学习。 思考 BN层对数据进行了归一化之后，为什么还要使用缩放参数和偏移参数进一步处理？ 某些情况下非标准化分布的层特征可能是最优的，标准化每一层的输出特征反而会使得网络的表达能力变得不好，作者为BN层加上了2个科学系的缩放参数$ \gamma$和$ \beta$来允许模型自适应地去调整层特征分布。 BN层训练和测试的区别 可能学完了上面的算法，你只是知道它的一个训练过程，一个网络一旦训练完了，就没有了min-batch这个概念了。测试阶段我们一般只输入一个测试样本，看看结果而已。 因此测试样本前向传导的时候，上面的均值u、标准差σ 要哪里来？其实网络一旦训练完毕，参数都是固定的，这个时候即使是每批训练样本进入网络，那么BN层计算的均值$ \mu$、和标准差$ \sigma$都是固定不变的。我们可以采用这些训练阶段的均值$ \mu$、和标准差$ \sigma$来作为测试样本所需要的均值、标准差，于是最后测试阶段的$ \mu $和$ \sigma$ 计算公式如下： $E[x] \leftarrow E_B[\mu_B]$ $Var[x] \leftarrow \frac{m}{m-1} E_B[\sigma_B^{2}]$ 上面简单理解就是： 直接计算所有batch平均值作为$\mu$； 采用每个batch的方差的平均值作为$\sigma$。 最后测试阶段，BN的使用公式就是：$y=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot x+\left(\beta-\frac{\gamma \mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)$ 也就是说， 在test的时候，BN用的是固定的mean和var, 而这个固定的mean和var是通过训练过程中对mean和var进行移动平均得到的，被称之为moving_mean和moving_var。 在实际操作中，每次训练时应当更新一下moving_mean和moving_var，然后把BN层的这些参数保存下来，留作测试和预测时使用。 BN层的作用？ 加快收敛速度 如果每一批次的数据分布不相同，那么网络就要在每次迭代的时候都去适应不同的分布，这样会大大降低网络的训练速度另一方面，均值为0方差为1的情况下，在梯度计算时会产生比较大的梯度值，可以加快参数的训练，更直观的来说，是把数据从饱和区拉到了非饱和区。更进一步这也可以很好地控制梯度爆炸和梯度消失现象。 防止过拟合（没有dropout/l1/l2正则那么效果明显） 在训练的时候，BN的使用使得一个mini-batch的所有样本都被关联在一起，因此网络不会从某一个训练样本中生成确定的结果。也就是说，同样一个样本的输出不再仅仅取决于样本本身，也取决于和这个样本同属于一个mini-batch的其它样本。 Dropout层什么是Dropout【训练阶段】在前向传播的时候，让某个神经元的激活值以一定的概率P停止工作。 【测试阶段】每一个神经元的参数都要乘以概率p。 Dropout作用缓解过拟合，提高网络模型的泛化性 为什么Dropout可以缓解过拟合 【bagging的角度】相当于取平均dropout掉不同的隐藏神经元就类似在训练不同的网络，整个droupout过程就相当于对很多个不同的神经网络取平均，而不同的网络产生不同的过拟合，一些互为“反向”的过拟合相互抵消就可以达到整体上的减少过拟合。 【正则的角度】减少神经元之间复杂的共适应关系dropout会导致两个神经元不一定每次都出现，所以权值的更新也不再依赖于有固定关系的隐含结点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。迫使网络取去学习更加鲁棒的特征。换句话说，假如神经元在做某种预测，那么它不应该对一些特定的线索片段太敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性），这个角度看dropout有点像l1，l2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性。]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大似然估计和EM算法]]></title>
    <url>%2F2018%2F06%2F01%2F%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8CEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文是对最大似然估计和EM算法做的一个总结。 一般来说，事件A发生的概率与某个未知参数$\theta​$有关，$\theta​$取值不同，则事件A发生的概率$p(A|\theta)​$也不同。当我们在一次实验中事件A发生了，则认为此时的$\theta​$值应是t的一切可能取值中使$p(A|\theta)​$最大的那个。最大似然估计就是要选取这样的t值作为参数t的估计值，使所选取的样本在被选的总体中出现的可能性为最大。 EM算法是在有潜变量的情况下，通过不断进行最大似然估计来求解参数的过程。 最大似然估计最大似然估计/极大似然估计(Maximum Likelihood Estimation，简称MLE) 前言利用已知的样本的结果，在使用某个模型的基础上，反推出最有可能导致这种结果的模型参数值。是一种参数估计方法。 例子: 定义有些绕口，下面我们通过例子来理解一下。 我们知道，现实中的硬币是均匀的，即抛出后正面朝上和反面朝上的概率是一样的。但是现在假设有两枚不均匀的硬币，这两枚硬币正面朝上的概率都不是0.5，分别记为$p_1$和$p_2$记每选用一枚硬币抛5次为一个实验，得到实验结果如下： 实验所选硬币 实验结果 1 3正、2反 1 1正、4反 1 2正、3反 2 2正、3反 2 1正、4反 好！那么我现在问，根据实验结果你可以得到$p_1$和$p2$的值吗？你应该会这样算： $p1=(3+1+2)/15=0.4$ $p2=(2+1)/10=0.3$ 然后你就说了，$p1$最有可能是0.4，$p2$最有可能是0.3。 现在我们就完成了一次最大似然估计！ 什么是似然估计？—-根据实验结果，反推出实验参数。 什么是最大似然估计？—-根据实验结果，反推出最有可能导致这个结果的实验参数。 什么是概率？—-根据参数，推出可能的实验结果。 用数学的语言来描述就是： 概率：$p(x|\theta)$ 在参数$\theta$确定的情况下，$x$出现的概率 似然：$L(\theta|x_1,x_2,…)$ 根据结果，反推参数 定义最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。 假设我们要统计全国人口的身高，首先假设这个身高服从正态分布，但是该分布的均值与方差未知。一种解决办法是：通过采样，获取部分人的身高，然后通过最大似然估计来获取上述假设中的正态分布与均值。 最大似然估计中采样需要满足：所有的采样都是独立同分布的。下面具体描述最大似然估计： 首先，假设$x_1,x_2,…,x_n$是独立同分布的采样，$\theta$是模型的参数，$f$为我们使用的模型。所以，参数为$\theta$的模型产生上述采样可以表示为： $f(x_1,x_2,…,x_n)=f(x_1|\theta)\times f(x_2|\theta)\times … \times f(x_n|\theta)$ 回到上面“模型已定，参数未知”的说法，此时，我们已知的为$x_1,x_2,…,x_n$，未知参数$\theta$，所以似然定义为： $L(\theta|x1,x_2,…,x_n)=f(x_1,x_2,…,x_n)=\prod\limits{i=1}^nf(x_i|\theta)$ 最大似然估计就是求上式的极值点。所以自然想到求导了，因为右边是连乘，为了计算简便，同时对等号两边取对数，有： $\ln L(\theta|x1,…,x_n)=\sum\limits{i=1}^n\ln f(x_i|\theta)$ $\hat l = \frac 1n\ln L$ 其中 $\ln L(\theta|x_1,…,x_n)$称为对数似然，$\hat l$为平均对数似然。通常所说的最大似然指的是最大的平均对数似然： $\hat{\theta}{mle}=\arg\limits{\theta\in\Theta}\max\hat l(\theta|x_1,…,x_n)$ 例子1举一个在很多博客都看到过的例子： 盒子里总共有若干个除颜色外均相同的球，进行100次有放回的随机摸球实验，摸到红球和白球的次数分别是30和70。用最大似然估计法求盒子中红球和白球比例。 解： 设红球比例为p，则白球为(1-p)。 则出现题目中结果(30次红，70次白)的概率可以写成： $f(x1,x_2,…,x{100}|\theta)=f(x1|\theta)\times f(x_2|\theta)\times …\times f(x{100}|\theta)$ $=p^{30}(1-p)^{70}$—————————————-式1 其中$x_i$代表第i次实验结果。 ps: 我一直觉得上面这个式子有问题….这是问题不考虑红球白球取出的次序，计算概率时不是应该再乘以一个$C_{100}^{30}$吗？ 因为常数系数不影响之后的求导结果，所以这个问题不影响下面计算，但还是很想知道为什么。。。 好，实验结果(抽100次，有30次红70次白)我们已经知道了，所以当理论上这个概率(上式的值)越大，说明实际情况发生的可能性也越大，实验结果符合预期岂不是美滋滋:happy:。 So，我们希望式1的值尽可能大。即让式1取最大值时，此时参数p的取值就时我们对p的最大似然估计。 那么，直接对式1求导就行了：$f^{‘} =0\Longrightarrow p =0.3 $。也就是说当p=0.3时，出现这种实验结果(30,70)的可能性最大。这和我们常识的推测一致。所以0.3是我们求得的参数p的最大似然值。 例子2 正态分布假如有一组采样值$(x_1,x_2,…,x_n)$，我们知道其服从正态分布，且标准差已知。当这个正态分布的期望为多少时，产生这个采样数据的概率为最大？ 这个例子中正态分布就是模型M，而期望就是前文提到的未知参数$\theta$。 似然：$L(\theta|x1,x_2,…,x_n)=f(x_1,x_2,…,x_n|\theta)=\prod\limits{i=1}^nf(x_i|\theta)$ 正态分布的公式：$M=f(x)=\frac1{\sqrt{2\pi}\sigma}\exp \left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$ $N(\mu,\sigma^2)$ 似然值：$f(x1,x_2,…,x_n|\theta)=\left(\frac1{\sqrt{2\pi}\sigma}\right)^n\exp\left(-\frac1{2\sigma^2}\sum\limits{i=1}^n(x-\mu)^2\right)$ 对上面式子求导可得：$l^{‘}=0\Longrightarrow \sum\limits{i=1}^n(x_i-\mu)=0\Longrightarrow\mu=\frac1n\sum\limits{i=1}^nx_i$ 最大似然算法推导出的正态分布的期望和我们尝试算出来的一样。 总结我们可以得到最大似然估计的算法步骤如下： 写出似然函数； 如果直接求导困难，则两边同时取$\ln$对数，化成对数似然函数； 求导； 根据导数=0，求出极值点。 EM算法EM(Expectation Maximal)算法，也称最大期望算法。 很接地气的EM算法解读 强烈推荐看上面这篇博客看，我觉得算法就是需要这种通俗的讲解才能真正吃透。这里就不累述了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>算法，机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向量范数和矩阵范数]]></title>
    <url>%2F2018%2F05%2F28%2F%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0%E5%92%8C%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0%2F</url>
    <content type="text"><![CDATA[范数范数分为向量范数和矩阵范数，概念经常忘记，这里总结一下。 向量范数对于向量$x=[x_1,x_2,…,x_N]$，其范数定义如下： p-范数$|x|p=(\sum{i=1}^N|x_i|^p )^{1/p}$ 对向量元素绝对值的p次方求和后，再计算1/p次幂。 特殊地，当p取0，1，2，$\infty$，$-\infty$，时，对应范数意义如下。 0-范数特殊地，数学中认为，向量的0范数即向量中非零元素个数。 1-范数$|x|1=\sum\limits{i=1}^N|x_i|$ 向量的1范数即向量中元素的绝对值之和。到原点的距离之和。 2-范数$|x|2=\left(\sum\limits{i=1}^N|x_i|^2\right)^{\frac12}$ 向量的2范数也称欧几里得范数，也就是通常说的向量长度。 $\infty$-范数$|x|\infty=\max\limits{i}|x_i|$ 向量的正无穷范数即向量元素绝对值中的最大值。到原点的最远距离。 $-\infty$-范数$|x|_{-\infty}=\max\limits_i|x_i|$ 向量的负无穷范数即向量元素绝对值中的最小值。到原点的最近距离。 矩阵范数对于矩阵$A=(a{ij}){m\ast n}$，其范数定义如下： 0-范数矩阵的0-范数同样标识矩阵中非零元素的个数。可以表示矩阵的稀疏程度。 1-范数$|A|1=\max\limits_j\sum\limits{i=1}^m|a_{ij}|$ 矩阵的1-范数，也称列和范数，即所有矩阵列向量的绝对值之和的最大值。 2-范数$|A|_2=\sqrt{\lambda_1}$，$\lambda_1$是$A^TA$的最大特征值(所以说方阵才有2-范数)。 矩阵的2-范数，也称谱范数，即$A^TA$的最大特征值开平方。 $\infty$-范数$|A|\infty=\max\limits_i\sum\limits{j=1}^m|a_{ij}|$ 矩阵的$\infty$-范数，也称行和范数，即所有矩阵行向量的绝对值之和的最大值。 F-范数$|A|F=\left(\sum\limits{i=1}^m\sum\limits{j=1}^na{ij}^2\right)^{\frac12}$ 矩阵的F-范数，即Frobenius范数，矩阵元素的平方和再开平方。]]></content>
      <categories>
        <category>概念</category>
      </categories>
      <tags>
        <tag>经常忘</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习基石》笔记三]]></title>
    <url>%2F2018%2F05%2F26%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E3%80%8B%E7%AC%94%E8%AE%B0%E4%B8%89%2F</url>
    <content type="text"><![CDATA[机器学习的分类机器学习可以按照下列四种情况进行分类： 输出空间y不同 数据标签$y_n$不同 输入数据的方式不同 输入空间$x$不同 这一章本来内容是很多的，但是老师讲的比较浅显易懂。笔记部分做的比较简单。 输出空间y不同二元分类(Binary classification) 也可认为是是非题，输出只有两个值。 上图都属于二元分类问题。 多分类问题(Multiclass classification)多分类是指输出值可能值为有限个，且大于2。 回归问题(Regression)回归问题的输出值是属于某个范围内的任意一个实数，是无限的。 结构学习(Structured learning) 总结 数据标记方法不同监督学习(Supervised learning)输入数据均有类型标识。 无监督学习(Unsupervised learning)输入数据没有类型等标识，例如：聚类算法。 半监督学习(Semi-supervised learning)仅对部分数据进行标识。 强化学习(Reinforcement learning)添加惩罚、奖励因子 总结 输入数据的方式不同批量学习(Batch learning)将训练数据一次性批量输入给学习算法。“填鸭式” 在线学习(Online learning)按照顺序输入训练数据，在这个过程中不断纠正模型。 主动学习(Active learning)可以参考这里 学习算法主动地提出要对哪些数据进行标注。 输入数据不同具体的特征(Concrete feature)特征的每一维都有它具体的意义。对机器学习来说，是一种很简单的特征。 原始特征(Raw features)例如，手写数字的识别问题 对于机器学习来说，比具体特征更复杂些，需要人类或机器转化成具体的特征。 抽象特征(Abstract feature)一些没有实际意义的特征]]></content>
      <tags>
        <tag>机器学习，笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年搭博客踩过的坑]]></title>
    <url>%2F2018%2F05%2F20%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E6%90%AD%E5%8D%9A%E5%AE%A2%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[此刻，我写下这篇博客是怀着一种“我想哭但是哭不出来”的心情。/(ㄒoㄒ)/~~ 博客搭建过程中踩了很多雷，虽然说网上有很多教程，但是可能由于各种原因会出现很多状况，有时一个小问题折腾了很多时间。所以，打算记录一下这个过程中遇到的一下问题。 资源汇总先给出一些我参考过的一些比较好的博客教程链接。基础+进阶 按照这个教程可以完成简单的博客搭建 很全的博客美化篇 博客美化篇 域名问题之前用的二级域名，然后买了一个域名，绑定教程真的很简单，然后不生效也是真的，搞了很久才发现，域名没有实名认证所以被暂停解析了啊….(′д｀ )…彡…彡 做好备份在跟着教程修改一些脚本文件的时候，不确定是哪里处理问题，本地模拟运行的时候控制台一直在报错，也不知道该怎么改回去，当时真的是欲哭无泪。所以说一定一定一定要备份好本地的博客文件夹。建议直接将博客文件夹部署到github。具体做法是，新建一个backup分支，每次做完本地的更改，git commit、git push将本地文件更新到github上。 熟悉github的同学，做这些还是很简单的，当然网上还是有很多教程。。 可以参考博客：链接 不蒜子问题我使用的是NexT主题，按理来说，直接更改\themes\next下的_config.yml的下面字段中第一个enable为true就可以了呀！！！！！！！！然而……. 123456789101112131415busuanzi_count: # count values only if the other configs are false enable: true # 这里是不蒜子统计的开关 # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; 访客数 site_uv_footer: 人 # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; 访问量 site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt;阅读量 page_pv_footer: 不知道什么原因，直接这样改的话我的博客会有小bug： 首页访客数和访客量均显示正常，转到其它页面后只有文字，没有数字。 如上图所示，阅读量完全不显示，而且位置错乱。 我强迫症真的好严重….一直折腾这些，然而几乎找遍全网，也没发现有人遇到这种问题….可我就是这么执着…. 终于！！！在经历了24小时的摸索之后，找到了解决方案如下： BUG1解决： 路径\themes\next\layout_third-party\analytics 下有一个 busuanzi-counter.swig文件，文件内容原来是这样的： 123456789101112131415161718192021&#123;% if theme.busuanzi_count.enable %&#125;&lt;div class=&quot;busuanzi-count&quot;&gt; &lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; &#123;% if theme.busuanzi_count.site_uv %&#125; &lt;span class=&quot;site-uv&quot;&gt; &#123;&#123; theme.busuanzi_count.site_uv_header &#125;&#125; &lt;span class=&quot;busuanzi-value&quot; id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt; &#123;&#123; theme.busuanzi_count.site_uv_footer &#125;&#125; &lt;/span&gt; &#123;% endif %&#125; &#123;% if theme.busuanzi_count.site_pv %&#125; &lt;span class=&quot;site-pv&quot;&gt; &#123;&#123; theme.busuanzi_count.site_pv_header &#125;&#125; &lt;span class=&quot;busuanzi-value&quot; id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt; &#123;&#123; theme.busuanzi_count.site_pv_footer &#125;&#125; &lt;/span&gt; &#123;% endif %&#125;&lt;/div&gt;&#123;% endif %&#125; 刚开始想也没错，感觉这部分脚本就是设置不蒜子统计结果的显示的。可是就是这么神奇，把这些内容剪切下来粘到themes\next\layout_partials\footer.swig的最后，bug1解决了……… BUG2解决： 也不算解决，我直接换成leancloud来统计文章阅读次数了…… 具体操作可以参考这个博客： 链接 数学公式Mathjax这里我也倒腾了好久，网上很多教程比较老了，建议尽量参考新一点的博客。 我最后做法很简单，直接在主题的配置文件内打开mathjax开关，然后如果某篇博客用到了公式的话，不要忘记在前面加上mathjax: true。如下123456---title: index.htmldate: 2018-05-20 10:01:30tags:mathjax: true-- 终于想起来了，当时就是因为在调数学公式的显示，根据网上某教程改了后直接崩了….所以说备份的重要性…… 2018.5.28更新： 事实证明，上面的简单改法并没有很好地work，偶尔还是会出现不能解析的错误。所以我开始再网上找新的解决方案。参考了好几多博客，大致明白了不能正确解析公式的原因。 原因 hexo默认使用hexo-renderer-marked引擎进行网页渲染，其中对许多字符诸如划线、下划线、中括号等定义了转义。因此，在进行网页渲染时，数学公式中的字符先通过hexo-renderer-marked进行转义，所以再通过Mathjax渲染出来的数学公式很有可能无法正常显示。 hexo-renderer-kramed是在hexo-renderer-marked基础上修复了一些bug，所以可以考虑用前者来代替后者。 所以修复方法如下。 修复方法 在博客根目录下运行命令npm install hexo-renderer-kramed --save； 在博客根目录下找到package.json文件，删除hexo-renderer-marked所在的行，不直接使用命令卸载的原因是，有可能其它重要的包会同时被删除； 在根目录下打开node_modules\kramed\lib\rules\inline.js，将第11行和第20行注释掉，做如下替换： 1234//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 这是为了将之前不合理的转义取消。 在根目录下打开node_modules\hexo-renderer-kramed\lib\renderer.js，找到下列内容，完成相应的注释和替换： 123456// Change inline math rulefunction formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ // return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$'); return text; &#125; 完成以上步骤，然后重新hexo g -d部署完成就可以看到公式正常现实啦~对了，之前说的在每篇博客前面添加mathjax: true不要忘了嗷~~ 评论系统经过综合考虑，选了valine。我觉得这个挺好的，不用登录也可以评论，评论的时候留下邮箱还可以收到回复提醒，还可以通过邮箱关联gravator头像。 现在觉得唯一有点不好的就是，当文章收到新评论的时候，博主怎么知道？？？得时刻关注leancloud里面的comment有没有增加吗？？？？ 具体设置方法可以参考这个博客： 链接 我的next主题版本比较新，上面这个博客“页面中的设置部分”，我只是在主题配置文件做了如下设置： 还有一点需要注意的是，因为设置好了安全域名，所以本地调试模式下评论系统不能正常使用，需要 hexo g -d 部署后再进行测试。 另外，有大佬做了一个邮箱提醒升级版，大家可以参考：valine评论系统的一种优化 暂时先这些，以后踩了坑再来填…..不对为什么还要踩坑QAQ。。。]]></content>
      <categories>
        <category>填坑</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>填坑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习基石》笔记二]]></title>
    <url>%2F2018%2F05%2F16%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E3%80%8B%E7%AC%94%E8%AE%B0%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[回顾上一章中讲到了一些机器学习的基本概念，通过介绍银行信用卡派发系统，我们大致了解了机器学习的过程。如下图所示，根据银行现有的数据集$D = {(x_1, y_1), (x_2, y_2), …, (x_n, y_n)}$，其中$x_n$是一个向量，表示第$n$个申请人的信息，$y_n$是一个二值(是/否)变量，表示是否给该人派发信用卡。我们认为，$f$是理论上正确的，可准确描述$x\longrightarrow y$这个过程的函数。 从图中我们可以看出用机器学习来决策是否给某位新用户$x$派发信用卡的过程为： 取已有的数据集作为训练集，运用算法$A$，从所有的假设函数中，选出性能最好的、最接近$f$的决策函数$g$ 那么，假设函数$h$又是什么呢？我们该如何选择？ PLA(perceptron learning algorithm)介绍 在信用卡派发问题中，我们认为顾客各项特征的重要性是不等的，所以给每项特征值赋予不同的权重，将特征加权求和结果作为得分score。选定一个阈值，当score大于这个阈值时，建议给该人派发信用卡，否则拒绝申请。 所以，我们的假设函数可以写成： $h(x) = sign((\sum_{i=1}^dw_ix_i)-threshold)$ 上式可进行如下转化。通过添加一个$x_0=+1$，将threshold作为其权重$w_0$，$h(x)$可写成下面的形式： 所以，$H$是一个函数集，不同的$w_0, w_1, … ,w_n$的组合确定了不同的假设函数$h(x)$。 对于一个新的需要决策的输入数据$x$，将其代入假设函数，得到的结果即为该感知器的决策。 下面我们来直观的理解一下$h(x)$。 我们可以想象每个$x$在平面内都有一个对应的位置，由于$y$值有两种，所以分别用‘x’和’o’分别标记这两类$x$。当$x=(x_1, x_2)$，是一个二维向量的时候，$h(x) = sign(w_0 + w_1x_1 + w_2x_2)$。这时，$h(x)$可以看成平面上的一条直线，这条直线可以将平面分成两部分，同理这些点也分成了两部分。 $h(x)$的目的就是要尽可能将’x’和’o’这两类点放在直线的两侧。下图中，我们可以看出，右边的直线比左边的更能y有效区分这些点。 所以，当使用某个$h(x)$来进行决策的时候，只需要看看输入数据在直线的哪一侧即可。 当$x$是三维的话，相当于在空间中找一个平面来划分点。同理，可以推广到多维空间。 所以，感知器可以看成是一种线性分类。 从$H$中选取$g$ 机器学习的目的是想要尽可能得到接近$f$的$g$，但是$f$是未知的，所以该如何确定$g$呢？答：从初始值$g_0$开始，在数据集$D$上不断纠正、优化$g_0$ 因为不同的$w$对应不同的$h(x)$，我们用权重向量$w_0$代表$g_0$。 对于$wt$，若发现在$(x{n(t)}, y{n(t)})$处预测结果有错，即 $$sign(w^T_tx{n(t)})\neq y_{n(t)}$$ 可以把上式中$sign(w^Ttx{n(t)})$看成是两个向量的内积，即$sign(w^Ttx{n(t)})=|wt^T||x{n(t)}|cos\theta$，所以我们将问题转化为判断向量夹角的预测是否正确。 那么$sign(w^Ttx{n(t)})&gt;0$表示预测结果两个向量的夹角为锐角，小于0表示预测结果夹角是钝角 情况1. 实际结果(y=+1)为锐角，预测结果(-1)为钝角 上图中，红色线条标记的$w$是预测值，黑色线条标记的$x$是输入数据，紫色线条标记的是纠正后的结果 纠正方法是：$w{t+1}\longleftarrow w_t+x{n(t)}$ 情况2. 实际结果(y=-1)为钝角，预测结果(+1)为锐角 上图中，红色线条标记的$w$是预测值，黑色线条标记的$x$是输入数据，紫色线条标记的是纠正后的结果 纠正方法是：$w{t+1}\longleftarrow w_t-x{n(t)}$ 综上：$w{t+1}\longleftarrow w_t+y{n(t)}x_{n(t)}$ 不断重复上述纠正，直到没有错误。 我理解的算法流程是： 对于一个初始选取的$w_0$，遍历所有数据判断是否预测正确. 若某个数据数据预测不正确的话，用$w{t+1}\longleftarrow w_t+y{n(t)}x_{n(t)}$来更新$w$ PLA终止条件PLA只能用来解决线性可分问题。如下图所示。 使用PLA来解决线性可分问题，PLA一定可以终止。 下面给出证明。 思路 要证明PLA能终止，也就是证明PLA能得到正确解，也就是证明向量$w_f$和$w_t$尽可能重合（其中t表示经过t次错误纠正后），即它们得夹角尽可能小。所以，为了排除向量长度的影响，我们将向量归一化，可以观察$\frac{w_f}{|w_f|}\cdot \frac{w_t}{|w_t|}$即夹角余弦值$cos\theta$的变化。 证明 数据集线性可分，也就是说一定存在$w_f$满足$y_n=sign(w_f^Tx_n)$，即$w_f$可对数据集进行完美预测 对于某个能正确预测得点$xn$，它到线的距离是$y{n(t)}w_f^Tx_n$，这个距离应该是大于0的，所以有： 上式中，$min_nw_f^Ty_nx_n​$即数据点到预测直线的最短距离。 所以： $wf^Tw_t=w_f^T(w{t-1}+y{n(t)}x{n(t)})$ $=wf^Tw{t-1}+wf^Ty{n(t)}x_{n(t)}$ $\geq wf^Tw{t-1}+min_nw_f^Ty_nx_n$ $\geq wf^Tw{t-2}+2*min_nw_f^Ty_nx_n$ $\geq w_f^Tw_0+t*min_nw_f^Ty_nx_n$ $\geq t*min_nw_f^Ty_nx_n$ 即 $w_f^Tw_t \geq t*min_nw_f^Ty_nx_n$ ——————(公式1) 当需要进行错误纠正时，一定是预测和实际结果相反了，也就是： 所以： $|wt|^2 \leq |w{t-1}|^2+max_n|x_n|^2 $ ​ $\leq |w_0|^2+t*max_n|x_n|^2$ ​ $\leq t*max_n|x_n|^2$ —————————(公式2)说明随着t增加，$w_t$的长度会慢慢增加 根据公式1和公式2可以对$\frac{w_f}{|w_f|}\cdot \frac{w_t}{|w_t|}$（也就是两个单位向量的数量积）进行如下推导： $\frac{w_f}{|w_f|}\cdot \frac{w_t}{|w_t|} =1 \ast 1 \ast cos \theta$ $\geq \frac{t * min_nw_f^Ty_nx_n}{|w_f||w_t|}$ $\geq \frac{t*min_nw_f^Ty_nx_n}{|w_f|\sqrt t \ast max_n}$ $\geq \frac {t * min_nw_f^Ty_nx_n} {|w_f| \sqrt{t} \ast max_n |x_n|}$ $\geq\sqrt t * \frac{min_nw_f^Ty_nx_n}{|w_f| \ast max_n|x_n|}$ 上式中$\theta$是$w_f$和经t次纠正后得到的$w_t$的夹角。所以最后可得： 随着纠正次数t得增加，夹角越来越小，说明对于线性可分问题PLA算法一定可以终止且能得到正确解。 总结 优点 容易实现；代码简单；任意维度的线性可分问题都能使用。 缺点 首先得假设数据集D是线性可分的； $w_f$未知，所以并不确定算法什么时候终止 Pocket algorithm背景现实中，并不是所有的问题都是线性可分的。数据是可能有噪声、误差影响的，这也会对线性可分问题造成干扰。 如何在有噪声的数据里找一条好的线呢？ 我们假设噪声很小（当然噪声很大的话，这个问题也就没有处理的意义了），虽然没办法找到一条不犯错误的线，但是我们可以找一条犯错尽可能少的线。 然而…这个问题是NP难的……. Pocket算法可用来求近似解。 Pocket 算法 类似贪心算法。 与PLA不同，Packet算法使用随机的方法来寻找错误，并且记录当前最好的解，通过不断更新这个最优解的取值。Packet算法没有明确的终止条件条件。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习基石》笔记一]]></title>
    <url>%2F2018%2F05%2F13%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E3%80%8B%E7%AC%94%E8%AE%B0%E4%B8%80%2F</url>
    <content type="text"><![CDATA[课程简介 what 什么是机器学习？ when 什么时候使用机器学习？ why 为什么可以使用机器学习？ how 机器学习是如何学的？ how 怎样优化机器学习？ 是什么学习我们看书，通过阅读，可以学会知识；小朋友学说话、走路都是一种学习的过程。所以我们通常说的学习是一种获取技能的过程，通过对观察不断地积累，从而获得技能经验。 observations $\longrightarrow$ learning $\longrightarrow$ skill 机器学习简单地说，机器学习就是我们希望用计算机来模拟人学习的过程。通过喂给计算机数据，让计算机学得技能，得到某一种表现的增进。 data $\longrightarrow$ ML $\longrightarrow$ imporoved/performance/measure 应用人可以很简单地辨认出图片里的一棵树，那要如何教计算机去辨认呢？是直接写100条规则作为辨识程序吗？这似乎有点难。我们知道，一个三岁小孩认识树，是通过不断的观察来学会的，而并不是套用100条规则。机器学习想要做的事也一样，通过学习来辨认。 卫星导航 视觉/语言辨认 high-frequency trading(例如股票交易需要很快做出决定) 针对消费者的营销也就是说，要授计算机以渔 三个关键点 模式既然有某种表现要被增进，所以应该存在一些隐藏的模式可以被学习 规则有清楚的规则 数据是机器学习的输入 基本组成以一个银行问题为例子，申请信用卡人数众多，根据申请者信息(包括年龄、性别、年薪、居住实践、工作时间、负债金额)可以使用机器学习来判断该给哪些人派发信用卡。 输入：$x \in X$（申请者）输出：$y \in Y$（是否应该给他提供信用卡）待学习的模式，即映射函数：$f: x \longrightarrow y$ （理想的信用卡提供函数）数据，即训练用例： $D = {(x_1, y_1), (x_2, y_2), …, (x_n, y_n)}$（银行拥有的历史数据）假设，也就是学习到的技能：$g: x \longrightarrow y$ （通过学习得到的映射函数，用来对新的数据进行决策和判断） ​ ${(x_n, y_n)} from f \longrightarrow ML \longrightarrow g $ 存在很多的$Hypothesis$，我们的目标就是从这些$H$中，找到最接近$f$的$g$。 和其它领域的关系机器学习 vs 数据挖掘 机器学习 用训练数据来计算最接近$f$的假设函数$g$ 数据挖掘 从很多的数据中来寻找感兴趣的特征和属性 关系 如果 感兴趣的属性 就是 假设函数$g$, 那么可以把机器学习等同于数据挖掘; 如果 感兴趣的属性 和 假设函数$g$有关, 那么数据挖掘可以为机器学习服务; 机器学习也可以作为数据挖掘的一种工具 。 在实际使用中，很难区分机器学习和数据挖掘。 机器学习 vs 人工智能 机器学习 用训练数据来计算最接近$f$的假设函数$g$ 人工智能 计算一些能展现智力的习惯 关系 当$g \approx f$时，就体现了一种智能，说明机器学习可以实现人工智能； 计算机下围棋的例子。传统的人工智能：用tree实现。机器学习实现的人工智能：通过从大量数据进行学习来实现。 机器学习是一种实现人工智能的方法 机器学习 vs 统计学 机器学习 用训练数据来计算最接近$f$的假设函数$g$ 统计学 用数据对不确定的过程做一个推断 关系 (1) $g$是对结果的一种推断；$f$是不确定的。所以我们可以认为统计学可以用来实现机器学习。 (2) 传统的统计学关注数学假设的结果，并不关心计算。 所以说，统计学为机器学习提供工具。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林轩田-《机器学习基石》笔记汇总(更新中)]]></title>
    <url>%2F2018%2F05%2F12%2F%E6%9E%97%E8%BD%A9%E7%94%B0-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E3%80%8B%E7%AC%94%E8%AE%B0%E6%B1%87%E6%80%BB-%E6%9B%B4%E6%96%B0%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[前言&emsp;&emsp;人工智能是当前科技热潮，以后就业还是向往这个方向发展的，机器学习是人工智能的一种工具。之前对机器学习的了解特别凌乱，所以我打算先把重新开始，先花半个月左右时间来梳理一下理论知识，主要是以台湾大学林轩田老师的《机器学习基石》视频课程（B站传送门,coursera传送门(1-8),coursera传送门(9-16)）为主，吴恩达老师的《machine learning》课程之前零零碎碎学到了第十二章，但感觉不如林老师的通俗易懂，所以打算学完之后再复习NG的课程作为理论巩固。后阶段主要做一些项目实战。 课程简介&emsp;&emsp;林老师的课程分为16次课，每次课分成四个视频小节。 The Learning Problem(机器学习是什么？它的应用以及和其它领域的联系) Learning to Answer Yes/No Types of Learning(二元分类、回归模型) Feasibility of Learning Training versus Testing Theory of Generalization The CV Dimension Noise and Error]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-021]]></title>
    <url>%2F2018%2F05%2F08%2Fleetcode-021%2F</url>
    <content type="text"><![CDATA[code everyday 002-021合并两个有序链表 方法一 递归的思路1234567891011121314151617181920212223242526/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123;//将l1 和l2 两个链表合并起来 //用递归的思路解决 if(l1 == nullptr) return l2; if(l2 == nullptr) return l1; ListNode *head; if(l1-&gt;val &lt;= l2-&gt;val)&#123; head = l1; head-&gt;next = mergeTwoLists(l1-&gt;next, l2); &#125; else&#123; head = l2; head-&gt;next = mergeTwoLists(l1, l2-&gt;next); &#125; return head; &#125;&#125;; 方法二 直接将小的插入到新链表中 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123;//将l1 和l2 两个链表合并起来 if(l1==nullptr) return l2; if(l2==nullptr) return l1; ListNode *p1, *p2, *head, *cur; p1 = l1; p2 = l2; head = p1; if(p1-&gt;val &lt;= p2-&gt;val)&#123; head = p1; p1 = p1-&gt;next; &#125; else&#123; head = p2; p2 = p2-&gt;next; &#125; cur = head; while(p1&amp;&amp;p2)&#123; if(p1-&gt;val &lt;= p2-&gt;val)&#123; cur-&gt;next = p1; cur = p1; p1 = p1-&gt;next; &#125; else&#123; cur-&gt;next = p2; cur = p2; p2 = p2-&gt;next; &#125; &#125; if(p1 == nullptr) cur-&gt;next = p2; else cur-&gt;next = p1; return head; &#125;&#125;; 下面是自己最初的解法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123;//将l1 和l2 两个链表合并起来 //返回l1 将l2插入到l1种 if(l1 == nullptr&amp;&amp;l2 != nullptr) return l2;//01 if(l2 == nullptr) return l1;//10 00 //l1和l2都非空 ListNode *p1,*p2; p1 = l1; p2 = l2; while(p1!=nullptr&amp;&amp;p2!=nullptr)&#123;//这个过程中l1是不断增加的(l2慢慢插入)，l2不断减小，p2始终指向l2头部， if(p2-&gt;val &gt;= p1-&gt;val)&#123; while(p1-&gt;next!=nullptr &amp;&amp; p1-&gt;next-&gt;val &lt;= p2-&gt;val) p1 = p1-&gt;next; ListNode *p11,*p22; if(p1-&gt;next == nullptr)&#123; p1-&gt;next = p2; return l1; &#125; p11=p1-&gt;next; p22 = p2; while(p22-&gt;next!=nullptr &amp;&amp; p22-&gt;next-&gt;val &lt;= p11-&gt;val) p22 = p22-&gt;next; //把l2中片段插入 p1-&gt;next = p2; if(p22-&gt;next == nullptr)&#123; p22-&gt;next = p11; return l1; &#125; p2 = p22-&gt;next; p22-&gt;next = p11; p1 = p11; &#125; else&#123;//p2-&gt;val &lt; p1-&gt;val ListNode *p22,*p11; p11 = p1; p22 = p2; while(p22-&gt;next!=nullptr &amp;&amp; p22-&gt;next-&gt;val &lt;= p1-&gt;val) p22 = p22-&gt;next; if(p22-&gt;next == nullptr)&#123; p22-&gt;next = p11; return p2; &#125; p1 = p11; l1 = p2; p22-&gt;next = p11; p2 = p22-&gt;next; &#125; &#125; if(p1 == nullptr) return l1; &#125;&#125;; 上面程序是自己写的，思路如下：p1始终指向要返回的最后链表的前面当l1为空的时候，直接返回l2；当l2为空的时候，直接返回l1;初始p1=l1,p2=l2当l1和l2都不为空，循环下面&emsp;&emsp;当 p1 &lt;= p2时，从p2找出一串介于p1和p1-&gt;next之间的数字串，插入其中，然后改变p2，p1位置&emsp;&emsp;当 p1 &gt; p2时，从 我写不下去了 之前写的好乱sad.]]></content>
      <categories>
        <category>刷题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>linked list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-083]]></title>
    <url>%2F2018%2F04%2F26%2Fleetcode-083%2F</url>
    <content type="text"><![CDATA[code everyday 001-083将有序链表中重复元素删除123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */ //给出一个已排好序的列表 整理成一个无重复元素的列表class Solution &#123;public: ListNode* deleteDuplicates(ListNode* head) &#123; ListNode *p, *q; if(head == NULL) return nullptr;//1.考虑初始链表为空的情况 p = q = head; if(q-&gt;next==NULL) return head;//2.考虑只有一个数的情况 p = q-&gt;next; while(p!=NULL)&#123; if(p-&gt;val == q-&gt;val)&#123;//如果和前一个数相等 跳过这个数 p = p-&gt;next; q-&gt;next = p; &#125; else&#123;//如果和前一个数不相等 q = p; p = p-&gt;next; &#125; &#125; return head; &#125;&#125;; 思路&amp;心得在原链表上进行操作，不论原链表是否为空，返回值始终是头节点；对于链表 口-&gt;口-&gt;口-&gt;口-&gt;口 前—&gt;后；设置两个指针p和q,p始终指向q后面一个节点；当p所指节点的值和q所指节点的值相等时，从链表删除p，且q不动，p指向之前的下一个节点;当p所指节点的值和q所指节点的值不相等时,p和q都向后挪一个；当p为空时，则所有节点判断完毕，退出循环。通过discuss区域查看别人代码发现，有种思路时，p和q不一定挨着，p不断向后推，当找到一个和q值不想同的位置时，再把p、q中间那段删了。 &emsp;&emsp;参考了知乎上很多前辈的建议，从今天起按照tag刷leetcode，从easy到hard。]]></content>
      <categories>
        <category>刷题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>linked list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法入门(持续更新)]]></title>
    <url>%2F2018%2F04%2F24%2FMarkdown%E8%AF%AD%E6%B3%95%E5%85%A5%E9%97%A8-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[Markdown 语法入门今天花了三个小时左右搭建好了博客，网上有很多教程，这里就不再累述过程了。Hexo + github部署博客，因为要用markdown写博客，所以总结一下markdown的语法。 常见语法 标题设置 一级标题二级标题三级标题 在标题文字前加”# “来标识。 通过#的个数来分辨标题级别。 ==#序列和文字间要有空格==，我们这里为了预览格式，没有添加空格。 最多有六级标题 列表设置1- * + 这三种符号都可以用来生成无序列表 有序列表则用数字加英文小数点来标识。如：11. 有序列表 引用设置用&gt;来标识。&gt;的个数标识引用的级数。一级引用 二级引用 三级引用… 强调设置1234567**文字内容** //加粗 __文字内容__ //加粗 *文字内容* //斜体 _文字内同_ //斜体 ==文字内容== //高亮 ~~文字内容~~ //删除文本 在上面画线的那种 ~文字内容~ 效果如下哈喽哈喽哈喽哈喽==哈喽==哈喽~哈喽~ 代码设置代码分为行内代码和代码块行内代码可以使用 我是代码 来标识，可嵌入在文字之中。代码块使用三个`包起来 ，分别占一行 表格设置表格对齐格式居中：:—-:居左：:—-居右：—-:经测试 上面这些-的个数至少为1就可以了例子:1234|标题1|标题2|标题3||:-|:-:|-:||左侧试文本|中测试文本|右测试文本||左侧试文本1|中测试文本1|右测试文本1| 效果如下：he|标题1|标题2|标题3||:-|:-:|-:||左侧试文本|中测试文本|右测试文本||左侧试文本1|中测试文本1|右测试文本1| 分割线设置在一行中用三个以上的*、-、_来建立一个分隔线，行内不能有其他东西。也可以在符号间插入空格。例子： 123***--- 短横线___ 下划线 效果如下： *]]></content>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
</search>
