[TOC]

# 从熵到交叉熵损失函数的理解

## 说在前面

首先，热力学中的“熵”和我们要说的机器学习中的也就是信息学中的“熵”是不一样的。记得高中化学老说说过，熵越大说明状态越不稳定，气态的熵就大于固态的熵。但是

现在要说的“熵”就不一样了。

本文主要的内容：`熵-->相对熵(KL散度)-->交叉熵`。先来总结一下：

* 熵（信息熵）

表示事件所含信息量的大小。熵越大，所含信息量越大。

* 相对熵（KL散度）

衡量两个分布的差异

* 交叉熵

$KL散度 = 交叉熵 - 信息熵$，所以当信息熵固定的时候，可以用交叉熵变化反应KL散度的变化。

`KL散度可以被用来计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。所以用交叉熵来当做代价。`

## 熵(Entropy)

随机变量$X$，其取值有${x_1, x_2, ...}$，称这些取值为不同的事件。

1. 信息量

$P(x_i) = P(X=x_i)$越小，也就是**事件发生的概率越小，我们认为该事件的信息量就越大。**

信息量的计算：$I(x_i) = -logP(x_i)$

2. 熵

熵：各类事件信息量的期望。

熵的计算：$H(X) = \sum P(x_i)I(x_i) = -\sum P(x_i)logP(x_i)$

## 相对熵(KL散度 Kullback-Leibler Divergence)

KL散度，也称KL距离，一般被用来计算两个分布的差异。KL散度不具有对称性。

* 离散变量A和B分布的差别

$D_{KL}(A \| B)=\sum P_{A}(x_i) \log (\frac{P_{A}(x_i)}{P_{B}(x_i)})=\sum P_{A}(x_i) \log (P_{A}(x_i))-\sum P_{A}(x_{i}) \log (P_{B}(x_{i}))$

* 连续变量A和B分布的差别

$D_{K L}(A \| B)=\int a(x) \log \left(\frac{a(x)}{b(x)}\right)$

观察上面公式可以知道：

1. 当$P(A) = P(B)$，即两个随机变量分布完全相同，KL散度等于0；
2. 注意离散事件的公式，减号的前一部分即随机变量A的熵的相反数。
3. $D_{KL}(A||B) \ne D_{KL}(B||A)$

## 交叉熵(Cross Entropy)

我们可以使用KL散度来度量两个分布之间的差异，为什么还需要交叉熵？

根据上面的推导，我们得到$D_{KL}(A \| B) = \sum P_{A}(x_i) \log (P_{A}(x_i))-\sum P_{A}(x_{i}) \log (P_{B}(x_{i}))$

即 $D_{KL}(A||B) = -H(A) + H(A, B)$

该公式说明，`KL散度 = - 熵 + 交叉熵 `，也就是熵，当熵固定的时候，我们要用KL散度来衡量两个分布的差异时，等价于用交叉熵来衡量。

交叉熵公式：$H(A, B) = -\sum P_A(x_i)logP_B(x_i)$

**注意，交叉熵和KL散度一样，不具有对称性**

## 机器学习中交叉熵的应用

机器学习的过程，就是希望模型上学到的分布$P_M$和真实数据的分布$P_R$（一般用训练集的分布$P_T$）越接近越好，所以，用KL散度来衡量这个差异，也就是说最小化$D_{KL}(P_T||P_M)$。

由上面的推导我们知道$D_{KL}(P_T||P_M) = -H(P_T) + H(P_T, P_M)$

对于训练集来说，其信息熵$H(P_T)$是固定的，所以，最小化$D_{KL}(P_T||P_M)$等价于最小化交叉熵$H(P_T, P_M)$.

## 参考博客

[知乎](https://www.zhihu.com/question/65288314/answer/244557337)