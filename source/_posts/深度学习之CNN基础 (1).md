---
title: 深度学习之CNN基础（1）
date: 2018-07-05 10:32:32
tags: [深度学习]
mathjax: true
---
# 深度学习之CNN基础

本文主要是对CS231N课程中有关CNN的各种层进行一个总结，截图来自课程或CS231N网易云课堂的视频。

## 卷积层

卷积层是CNN中最基础的层，在DNN中，我们是考虑对数据的线性变换，而这里，CNN则考虑的是对图像（张量数据）的线性变换，DNN中并没有考虑空间信息，而CNN中的卷积操作，更好地来处理图像的这种空间性。

### 什么是卷积核？

卷积层涉及到的主要是用**卷积核**对图像进行卷积处理，卷积核说白了，就是三维的矩阵，用这个三维矩阵在feature map 上进行滑窗处理。

注意：用卷积核对图像（feature map）进行处理，输出的feature map的维度 和 卷积核的个数相等。比如说，用5\*5\*3（这里3是卷积核的数量）的卷积核对图像进行操作，输出的feature map 的维度一定是3.

### 卷积运算

![深度学习之CNN_1](..\images\深度学习之CNN_1.png)

如上图所示，图像大小是（32\*32\*3），用大小为（5\*5\*5）的卷积核处理，得到（28\*28\*5）的输出。

![深度学习之CNN_2](..\images\深度学习之CNN_2.png)

由上图可以知道，卷积输出的深度(激活映射的层数)和卷积核的个数相等。
每个卷积核都要在输入的整个深度上做卷积。

用一个有5\*5\*3个参数值的滤波器，然后以相同的方式滑动(卷积核)，划过整个的输入部分，从而得到一张激活映射(activation map).

![无padding操作时的卷积计算](..\images\深度学习之CNN_3.png)

这里，b是bias，这里的w是指将卷积核展开得到的一个一维向量。所以说，卷积操作本质还是一种线性变换。每一个卷积核会产生一个activation map，就是说生成的激活映射的层数和卷积核的个数 一样。

那么，我们知道，卷积操作的输出维度和卷积核的个数相等，但是，如何来确定生成的feature map的长宽呢？

![深度学习之CNN_4](..\images\深度学习之CNN_4.png)

* 在无边缘填充的情况下，对一张输入大小为N\*N的图像，使用大小为F*F的卷积核以步长为stride对它做卷积，输出图像的大小为  (N - F) / stride + 1(结果必须是整数，所以步长的选择必须适当)
* 然而，在实际的操作中，一般会对待进行卷积操作的图像进行边缘填充（填充方法可以是常用的零填充或者是其它），所以此时在计算输出图像的大小尺寸时，仍然时使用的之前的公式：
  (N - F) / stride + 1，但是N要进行更新，应该是填充后的大小。如下图所示：

![有padding操作时的卷积计算](..\images\深度学习之CNN_5.png)

### 思考

* **为什么要做零填充？**

 因为我们想尽可能地保持全尺寸输出。如果不做零填充或者其它任何形式的填充，那么输出图像的尺寸会迅速减小，这样是不好的，可能会损失很多的信息，最后只能用很少的值来表示原始图像。关于图像的边角信息也会损失的更多。

## 池化层

* 池化层的作用：下采样。
* 池化层的参数：**没有需要学习的参数**。需要设置池化的步长。
* 池化层的类型：最大池化、平均池化等。

![深度学习之CNN_6.png](..\images\深度学习之CNN_6.png)

## BN层

BN层的全称是Batch Normalization(批归一化层)，这里的batch，顾名思义，指的是我们mini-batch SGD中的 batch。

### BN层操作

首先是计算出mini-batch中各个特征的均值和方差，然后对mini-batch中的每个特征x_i 进行标准化处理，最后利用缩放参数$\gamma$和偏移参数$\ \beta$对特征$x^i$进一步处理后，来代替最初的输入特征值。也就是说对于每个神经元都会有一对参数值(（γ和β）通过训练来学习。

![深度学习之CNN_7.png](..\images\深度学习之CNN_7.png)

### 思考

* **BN层对数据进行了归一化之后，为什么还要使用缩放参数和偏移参数进一步处理？**

某些情况下非标准化分布的层特征可能是最优的，标准化每一层的输出特征反而会使得网络的表达能力变得不好，作者为BN层加上了2个科学系的缩放参数$ \gamma$和$ \beta$来允许模型自适应地去调整层特征分布。

* **BN层训练和测试的区别**

可能学完了上面的算法，你只是知道它的一个训练过程，**一个网络一旦训练完了，就没有了min-batch**这个概念了。测试阶段我们一般只输入一个测试样本，看看结果而已。

因此**测试样本前向传导的时候，上面的均值u、标准差σ 要哪里来？**其实网络一旦训练完毕，参数都是固定的，这个时候即使是每批训练样本进入网络，那么BN层计算的均值$ \mu$、和标准差$ \sigma$都是固定不变的。我们可以采用这些训练阶段的均值$ \mu$、和标准差$ \sigma$来作为测试样本所需要的均值、标准差，于是最后测试阶段的$ \mu $和$ \sigma$ 计算公式如下：

$\begin{aligned} \mathrm{E}[x] & \leftarrow \mathrm{E}_{\mathcal{B}}\left[\mu_{\mathcal{B}}\right] \\ \operatorname{Var}[x] & \leftarrow \frac{m}{m-1} \mathrm{E}_{\mathcal{B}}\left[\sigma_{\mathcal{B}}^{2}\right] \end{aligned}$

上面简单理解就是：

直接计算所有batch平均值作为$\mu$；

采用每个batch的方差的平均值作为$\sigma$。

最后测试阶段，BN的使用公式就是：$y=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot x+\left(\beta-\frac{\gamma \mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)$

`		也就是说， 在test的时候，BN用的是固定的mean和var, 而这个固定的mean和var是通过训练过程中对mean和var进行移动平均得到的，被称之为moving_mean和moving_var。
在实际操作中，每次训练时应当更新一下moving_mean和moving_var，然后把BN层的这些参数保存下来，留作测试和预测时使用。`

* **BN层的作用？**

1. 加快收敛速度

![深度学习之CNN_8.png](..\images\深度学习之CNN_8.png)

如果每一批次的数据分布不相同，那么网络就要在每次迭代的时候都去适应不同的分布，这样会大大降低网络的训练速度
另一方面，均值为0方差为1的情况下，在梯度计算时会产生比较大的梯度值，可以加快参数的训练，更直观的来说，是把数据从饱和区拉到了非饱和区。更进一步这也可以很好地控制梯度爆炸和梯度消失现象。

2. 防止过拟合（没有dropout/l1/l2正则那么效果明显）

在训练的时候，BN的使用使得一个mini-batch的所有样本都被关联在一起，因此网络不会从某一个训练样本中生成确定的结果。
也就是说，同样一个样本的输出不再仅仅取决于样本本身，也取决于和这个样本同属于一个mini-batch的其它样本。

## Dropout层

### 什么是Dropout

	【训练阶段】在前向传播的时候，让某个神经元的激活值以一定的概率P停止工作。
	【测试阶段】每一个神经元的参数都要乘以概率p。
### Dropout作用

缓解过拟合，提高网络模型的泛化性

### 为什么Dropout可以缓解过拟合

* 【bagging的角度】相当于取平均
dropout掉不同的隐藏神经元就类似在训练不同的网络，整个droupout过程就相当于对很多个不同的神经网络取平均，而不同的网络产生不同的过拟合，一些互为“反向”的过拟合相互抵消就可以达到整体上的减少过拟合。
* 【正则的角度】减少神经元之间复杂的共适应关系
dropout会导致两个神经元不一定每次都出现，所以权值的更新也不再依赖于有固定关系的隐含结点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。迫使网络取去学习更加鲁棒的特征。
换句话说，假如神经元在做某种预测，那么它不应该对一些特定的线索片段太敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性），这个角度看dropout有点像l1，l2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性。