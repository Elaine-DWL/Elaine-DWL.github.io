---
title: 《机器学习基石》笔记二
date: 2018-05-16 21:51:53
tags: [机器学习,笔记]
mathjax: true
---
# 回顾

上一章中讲到了一些机器学习的基本概念，通过介绍银行信用卡派发系统，我们大致了解了机器学习的过程。如下图所示，根据银行现有的数据集$D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$，其中$x_n$是一个向量，表示第$n$个申请人的信息，$y_n$是一个二值(是/否)变量，表示是否给该人派发信用卡。我们认为，$f$是理论上正确的，可准确描述$x\longrightarrow y$这个过程的函数。

![](/images/TIM截图20180515115344.png)

从图中我们可以看出用机器学习来决策是否给某位新用户$x$派发信用卡的过程为：

> **取已有的数据集作为训练集，运用算法$A$，从所有的假设函数中，选出性能最好的、最接近$f$的决策函数$g$**

那么，假设函数$h$又是什么呢？我们该如何选择？

# PLA(perceptron learning algorithm)

## 介绍

![](/images/TIM截图20180515134337.png)

在信用卡派发问题中，我们认为顾客各项特征的重要性是不等的，所以给每项特征值赋予不同的权重，将特征加权求和结果作为得分score。选定一个阈值，当score大于这个阈值时，建议给该人派发信用卡，否则拒绝申请。

所以，我们的假设函数可以写成：
> $h(x) = sign((\sum_{i=1}^dw_ix_i)-threshold)$

上式可进行如下转化。通过添加一个$x_0=+1$，将threshold作为其权重$w_0$，$h(x)$可写成下面的形式：

![](/images/TIM截图20180515141925.png)

所以，$H$是一个函数集，不同的$w_0, w_1, ... ,w_n$的组合确定了不同的假设函数$h(x)$。

对于一个新的需要决策的输入数据$x$，将其代入假设函数，得到的结果即为该感知器的决策。

下面我们来直观的理解一下$h(x)$。

我们可以想象每个$x$在平面内都有一个对应的位置，由于$y$值有两种，所以分别用‘x’和'o'分别标记这两类$x$。当$x=(x_1, x_2)$，是一个二维向量的时候，$h(x) = sign(w_0 + w_1x_1 + w_2x_2)$。这时，$h(x)$可以看成平面上的一条直线，这条直线可以将平面分成两部分，同理这些点也分成了两部分。

$h(x)$的目的就是要尽可能将'x'和'o'这两类点放在直线的两侧。下图中，我们可以看出，右边的直线比左边的更能y有效区分这些点。

![](/images/TIM截图20180515150502.png)

所以，当使用某个$h(x)$来进行决策的时候，只需要看看输入数据在直线的哪一侧即可。

当$x$是三维的话，相当于在空间中找一个平面来划分点。同理，可以推广到多维空间。

![](/images/TIM截图20180515152628.png)

所以，感知器可以看成是一种线性分类。

## 从$H$中选取$g$

> 机器学习的目的是想要尽可能得到接近$f$的$g$，但是$f$是未知的，所以该如何确定$g$呢？
> 答：**从初始值$g_0$开始，在数据集$D$上不断纠正、优化$g_0$**

因为不同的$w$对应不同的$h(x)$，我们用权重向量$w_0$代表$g_0$。

![](/images/TIM截图20180521122438.png)

对于$w_t$，若发现在$(x_{n(t)}, y_{n(t)})$处预测结果有错，即         $$sign(w^T_tx_{n(t)})\neq y_{n(t)}$$

可以把上式中$sign(w^T_tx_{n(t)})$看成是两个向量的内积，即$sign(w^T_tx_{n(t)})=|w_t^T||x_{n(t)}|cos\theta$，所以我们**将问题转化为判断向量夹角的预测是否正确**。

那么$sign(w^T_tx_{n(t)})>0$表示预测结果两个向量的夹角为锐角，小于0表示预测结果夹角是钝角

* 情况1. 实际结果(y=+1)为锐角，预测结果(-1)为钝角

![](/images/TIM截图20180521124616.png)

上图中，红色线条标记的$w$是预测值，黑色线条标记的$x$是输入数据，紫色线条标记的是纠正后的结果

纠正方法是：$w_{t+1}\longleftarrow w_t+x_{n(t)}$

* 情况2. 实际结果(y=-1)为钝角，预测结果(+1)为锐角

![](/images/TIM截图20180521140128.png)

上图中，红色线条标记的$w$是预测值，黑色线条标记的$x$是输入数据，紫色线条标记的是纠正后的结果

纠正方法是：$w_{t+1}\longleftarrow w_t-x_{n(t)}$

> 综上：$w_{t+1}\longleftarrow w_t+y_{n(t)}x_{n(t)}$

不断重复上述纠正，直到没有错误。

> 我理解的算法流程是：
>
> 对于一个初始选取的$w_0$，遍历所有数据判断是否预测正确.
>
> 若某个数据数据预测不正确的话，用$w_{t+1}\longleftarrow w_t+y_{n(t)}x_{n(t)}$来更新$w$

![](/images/TIM截图20180521141732.png)

## PLA终止条件

PLA只能用来解决**线性可分**问题。如下图所示。

![](/images/TIM截图20180524123014.png)

**使用PLA来解决线性可分问题，PLA一定可以终止**。

下面给出证明。

* 思路

要证明PLA能终止，也就是证明PLA能得到正确解，也就是证明向量$w_f$和$w_t$尽可能重合（其中t表示经过t次错误纠正后），即它们得夹角尽可能小。所以，为了排除向量长度的影响，我们将向量归一化，可以观察$\frac{w_f}{\|w_f\|}\cdot \frac{w_t}{\|w_t\|}$即夹角余弦值$cos\theta$的变化。

* 证明

数据集线性可分，也就是说一定存在$w_f$满足$y_n=sign(w_f^Tx_n)$，即**$w_f$可对数据集进行完美预测**

对于某个能正确预测得点$x_n$，它到线的距离是$y_{n(t)}w_f^Tx_n$，这个距离应该是大于0的，所以有：

![](/images/TIM截图20180524122448.png)

上式中，$min_nw_f^Ty_nx_n​$即数据点到预测直线的最短距离。

所以：

$w_f^Tw_t=w_f^T(w_{t-1}+y_{n(t)}x_{n(t)})$

$=w_f^Tw_{t-1}+w_f^Ty_{n(t)}x_{n(t)}$

$\geq w_f^Tw_{t-1}+min_nw_f^Ty_nx_n$

$\geq w_f^Tw_{t-2}+2*min_nw_f^Ty_nx_n$

$\geq w_f^Tw_0+t*min_nw_f^Ty_nx_n$

$\geq t*min_nw_f^Ty_nx_n$

即  $w_f^Tw_t \geq t*min_nw_f^Ty_nx_n$   ------------(公式1)

当需要进行错误纠正时，一定是预测和实际结果相反了，也就是：

![](/images/TIM截图20180524144208.png)

所以：

![](/images/TIM截图20180524144329.png)

$\|w_t\|^2 \leq \|w_{t-1}\|^2+max_n\|x_n\|^2 $

​            $\leq \|w_0\|^2+t*max_n\|x_n\|^2$

​            $\leq t*max_n\|x_n\|^2$   ------------------(公式2)说明随着t增加，$w_t$的长度会慢慢增加

根据公式1和公式2可以对$\frac{w_f}{\|w_f\|}\cdot \frac{w_t}{\|w_t\|}$（也就是两个单位向量的数量积）进行如下推导：

$\frac{w_f}{\|w_f\|}\cdot \frac{w_t}{\|w_t\|} =1 \ast 1 \ast cos \theta$

$\geq \frac{t * min_nw_f^Ty_nx_n}{\|w_f\|\|w_t\|}$

$\geq \frac{t*min_nw_f^Ty_nx_n}{\|w_f\|\sqrt t \ast max_n}$

$\geq \frac {t * min_nw_f^Ty_nx_n} {\|w_f\| \sqrt{t} \ast max_n \|x_n\|}$

$\geq\sqrt t * \frac{min_nw_f^Ty_nx_n}{\|w_f\| \ast max_n\|x_n\|}$


上式中$\theta$是$w_f$和经t次纠正后得到的$w_t$的夹角。
所以最后可得：


![](/images/TIM截图20180524160044.png)

**随着纠正次数t得增加，夹角越来越小，说明对于线性可分问题PLA算法一定可以终止且能得到正确解。**

## 总结

* 优点

容易实现；代码简单；任意维度的线性可分问题都能使用。

* 缺点

首先得假设数据集D是线性可分的；

$w_f$未知，所以并不确定算法什么时候终止

# Pocket algorithm

## 背景

现实中，并不是所有的问题都是线性可分的。数据是可能有噪声、误差影响的，这也会对线性可分问题造成干扰。

如何在有噪声的数据里找一条好的线呢？

我们假设噪声很小（当然噪声很大的话，这个问题也就没有处理的意义了），虽然没办法找到一条不犯错误的线，但是我们可以找一条犯错尽可能少的线。
![](/images/TIM截图20180524171429.png)

然而...这个问题是NP难的.......

Pocket算法可用来求近似解。

## Pocket 算法

![](/images/TIM截图20180524173637.png)

类似贪心算法。

与PLA不同，Packet算法使用随机的方法来寻找错误，并且记录当前最好的解，通过不断更新这个最优解的取值。Packet算法没有明确的终止条件条件。